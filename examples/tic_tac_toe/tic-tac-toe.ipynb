{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRYPCmc9lSxQ"
      },
      "source": [
        "To train this agent, click **Runtime** > **Run all**. Make sure you've enabled a free Tesla T4 GPU!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://github.com/openpipe/art\"><img src=\"https://github.com/openpipe/art/raw/main/assets/ART_pill.png\" height=\"50\"></a>\n",
        "<a href=\"https://discord.gg/zbBHRUpwf4\"><img src=\"https://github.com/openpipe/art/raw/main/assets/Discord.png\" height=\"50\"></a>\n",
        "<a href=\"https://art.openpipe.ai\"><img src=\"https://github.com/openpipe/art/raw/main/assets/Documentation_pill.png\" height=\"50\"></a>\n",
        "\n",
        "Questions? Join the Discord and ask away! For feature requests or to leave a star, visit our [GitHub](https://github.com/openpipe/art).\n",
        "\n",
        "</div>\n",
        "\n",
        "<a href=\"https://art.openpipe.ai/\"><img src=\"https://github.com/openpipe/art/raw/main/assets/Header_separator.png\" height=\"5\"></a>\n",
        "\n",
        "This notebook shows how to train a Qwen 2.5 3B model to play tic tac toe. It will demonstrate how to set up a multi-turn agent, how to train it, and how to evaluate it.\n",
        "\n",
        "Completions and metrics will be logged to Weights & Biases.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTxrhQzUlSxX"
      },
      "source": [
        "### Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxCwzb47lSxY"
      },
      "outputs": [],
      "source": [
        "# Portions adapted from Unsloth Notebooks (https://github.com/unslothai/notebooks)\n",
        "# Copyright (c) Unsloth contributors.\n",
        "# License: GNU LGPL v3.0.\n",
        "# Modifications by OpenPipe:\n",
        "# - switched to uv\n",
        "# - changed vllm/triton pinning logic\n",
        "# - added protobuf pins\n",
        "# See /licenses/LGPL-3.0.txt and /licenses/GPL-3.0.txt for full text.\n",
        "\n",
        "%%capture\n",
        "import os\n",
        "\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !uv pip install openpipe-art[backend]==0.4.11 --prerelease allow --no-cache-dir\n",
        "else:\n",
        "    try:\n",
        "        import numpy\n",
        "\n",
        "        get_numpy = f\"numpy=={numpy.__version__}\"\n",
        "    except:\n",
        "        get_numpy = \"numpy\"\n",
        "    try:\n",
        "        import subprocess\n",
        "\n",
        "        is_t4 = \"Tesla T4\" in str(subprocess.check_output([\"nvidia-smi\"]))\n",
        "    except:\n",
        "        is_t4 = False\n",
        "    get_vllm, get_triton = (\n",
        "        (\"vllm==0.9.2\", \"triton==3.2.0\") if is_t4 else (\"vllm\", \"triton\")\n",
        "    )\n",
        "    !uv pip install --upgrade \\\n",
        "        openpipe-art[backend]==0.4.11 protobuf==5.29.5 {get_vllm} {get_numpy} --prerelease allow --no-cache-dir\n",
        "    !uv pip install -qqq {get_triton}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/PrimeIntellect-ai/prime-cli.git\n",
        "import os\n",
        "os.environ[\"PRIME_API_KEY\"] = \"PLACHOLDER_KEY\"\n",
        "!prime env install m8ngotree/sudoku\n",
        "import verifiers as vf"
      ],
      "metadata": {
        "id": "voXjGYPzlXbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"verifiers[all]\""
      ],
      "metadata": {
        "id": "FssuuagPrsni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgbQxu2dlSxa"
      },
      "source": [
        "### Environment Variables\n",
        "\n",
        "Later on in the notebook, we'll be creating a model that can automatically logs metrics and chat completions to Weights & Biases. In order to do so, you'll need to provide your Weights & Biases API key as an environment variable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5XTMa3WBlSxb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Optional\n",
        "WANDB_API_KEY = \"\"\n",
        "if WANDB_API_KEY:\n",
        "    os.environ[\"WANDB_API_KEY\"] = WANDB_API_KEY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nxm8tjislSxb"
      },
      "source": [
        "### Agentic Environment\n",
        "\n",
        "<a name=\"Environment\"></a>\n",
        "\n",
        "ART allows your agent to learn by interacting with its environment. In this example, we'll create an environment in which the agent can play tic tac toe.\n",
        "\n",
        "Feel free to read as much or as little of this section's code as you'd like. The important thing to understand is that we're defining the rules of this agent's environment. In many cases, this will already be defined by the task you're trying to solve, but if you need to define a custom environment, this is how you do it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yaz2E9ROlSxc"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Login using e.g. `huggingface-cli login` to access this dataset\n",
        "ds = load_dataset(\"sapientinc/sudoku-extreme\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# my_math_env.py\n",
        "import verifiers as vf\n",
        "\n",
        "def load_environment(**kwargs):\n",
        "    \"\"\"Load and configure the environment.\"\"\"\n",
        "    # 1. Load dataset\n",
        "    dataset = ds\n",
        "\n",
        "    # 2. Configure parser\n",
        "    parser = vf.ThinkParser()\n",
        "\n",
        "    # 3. Define reward functions -- can automatically reference:\n",
        "    # - parser, prompt, completion, answer, state , task, info\n",
        "    def correct_answer(parser, completion, answer):\n",
        "        response = parser.parse_answer(completion) or ''\n",
        "        return 1.0 if response.strip() == answer.strip() else 0.0\n",
        "\n",
        "    # 4. Create rubric\n",
        "    rubric = vf.Rubric(\n",
        "        funcs=[correct_answer, parser.get_format_reward_func()],\n",
        "        weights=[1.0, 0.2]\n",
        "    )\n",
        "\n",
        "    # 5. Return configured environment\n",
        "    return vf.SingleTurnEnv(\n",
        "        dataset=dataset,\n",
        "        system_prompt=\"Think step-by-step, then give your answer.\",\n",
        "        parser=parser,\n",
        "        rubric=rubric,\n",
        "        **kwargs  # Pass through additional arguments\n",
        "    )"
      ],
      "metadata": {
        "id": "aK_MdeNRltKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gu0PkWMxtWk_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tp37cVValSxe"
      },
      "source": [
        "### Creating a Model\n",
        "\n",
        "Now that we've defined the rules of our environment, we can create a model that will learn to play 2048. We'll use a Qwen 2.5 3B model for this example. The `name` parameter will be associated with a wandb run, and the `base_model` parameter is the model that we'll be training a LoRA on top of.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5_eTlhMlSxf"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "import art\n",
        "from art.local import LocalBackend\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "backend = LocalBackend(path=\"./.art\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sAJTEV1lSxf"
      },
      "source": [
        "### Creating a Model\n",
        "\n",
        "Now that we've defined the rules of our environment, we can create a model that will learn to play tic tac toe. We'll use a Qwen 2.5 3B model for this example. The `name` parameter will be associated with a wandb run, and the `base_model` parameter is the model that we'll be training a LoRA on top of.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiiEfyBolSxg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "model = art.TrainableModel(\n",
        "    name=\"001-script\",\n",
        "    project=\"tic-tac-toe\",\n",
        "    base_model=\"Qwen/Qwen2.5-3B-Instruct\",\n",
        ")\n",
        "await model.register(backend)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\")"
      ],
      "metadata": {
        "id": "gGd1iV49sCI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import verifiers as vf\n",
        "\n",
        "# 1. Create environment\n",
        "env = vf.load_environment(\"sudoku\")\n",
        "\n",
        "# 2. Load model\n",
        "model, tokenizer = vf.get_model_and_tokenizer(\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
        "\n",
        "# 3. Configure training\n",
        "args = vf.grpo_defaults(run_name=\"my-experiment\")\n",
        "\n",
        "# 4. Train\n",
        "trainer = vf.GRPOTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,\n",
        "    env=env,\n",
        "    args=args,\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "rI2SSpmWr38N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ueQv20ylSxg"
      },
      "source": [
        "### Defining a Rollout\n",
        "\n",
        "<a name=\"Rollout\"></a>\n",
        "\n",
        "A rollout is a single episode of an agent performing its task. It generates one or more trajectories, which are lists of messages and choices.\n",
        "\n",
        "In this example, the rollout function generates a game of tic tac toe, and the agent plays it until the game is finished. It then returns a trajectory which contains all the `system` and `user` messages presented to the agent, as well as all the `choices` that the agent made.\n",
        "\n",
        "When the game is finished the `reward` for the agent's performance is calculated based on whether the agent won, lost, drew, or errored, which is then assigned to the trajectory.\n",
        "\n",
        "This rollout function will be called many times in parallel during each step of the training loop.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mS-Ed5Z5lSxh"
      },
      "outputs": [],
      "source": [
        "from verifiers.types import Messages, State\n",
        "from typing import Tuple\n",
        "\n",
        "class MyGameEnv(vf.MultiTurnEnv):\n",
        "\n",
        "    async def env_response(self, messages: Messages, state: State) -> Tuple[Messages, State]:\n",
        "        \"\"\"Define how the environment responds.\"\"\"\n",
        "        # Get the last message from the assistant\n",
        "        last_msg = messages[-1]\n",
        "        if last_msg[\"role\"] == \"assistant\":\n",
        "            player_action = last_msg[\"content\"]\n",
        "        else:\n",
        "            return [], state  # No response if not assistant message\n",
        "\n",
        "        # Check game state\n",
        "        if self.is_game_over(state):\n",
        "            response = [{\"role\": \"user\", \"content\": \"Game over!\"}]\n",
        "            state[\"done\"] = True\n",
        "            return response, state\n",
        "\n",
        "        # Update game state\n",
        "        state = self.update_state(state, player_action)\n",
        "        feedback = self.get_game_feedback(state)\n",
        "\n",
        "        # Return list of ChatMessage dicts\n",
        "        response = [{\"role\": \"user\", \"content\": feedback}]\n",
        "        return response, state\n",
        "\n",
        "def load_environment(**kwargs):\n",
        "    return MyGameEnv(dataset=ds, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Od4Esg1tlSxh"
      },
      "source": [
        "<a name=\"Loop\"></a>\n",
        "\n",
        "### Training Loop\n",
        "\n",
        "The training loop is where the magic happens. For each of the 100 steps defined below, the rollout function will be called 200 times in parallel. This means that 200 games will be played at once. Each game will produce a trajectory, which will be used to update the model.\n",
        "\n",
        "The `gather` step will wait for all of the trajectories to be generated, then it will delete all but the most recent checkpoint and train the model on the new trajectories.\n",
        "\n",
        "Inference will be blocked until the training is complete.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmWTDSFglSxi"
      },
      "outputs": [],
      "source": [
        "TRAINING_STEPS = 2\n",
        "ROLLOUTS_PER_STEP = 48\n",
        "LEARNING_RATE = 5e-5\n",
        "\n",
        "for i in range(await model.get_step(), TRAINING_STEPS):\n",
        "    train_groups = await art.gather_trajectory_groups(\n",
        "        (\n",
        "            art.TrajectoryGroup(\n",
        "                rollout(model, TicTacToeScenario(step=i))\n",
        "                for _ in range(ROLLOUTS_PER_STEP)\n",
        "            )\n",
        "            for _ in range(1)\n",
        "        ),\n",
        "        pbar_desc=\"gather\",\n",
        "    )\n",
        "    await model.delete_checkpoints()\n",
        "    await model.train(train_groups, config=art.TrainConfig(learning_rate=LEARNING_RATE))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsRir1xvlSxi"
      },
      "source": [
        "### Using the Model\n",
        "\n",
        "Just like that, you've trained an agent to play tic tac toe! Now it's time to use your model outside of ART, in the wild! The easiest way to do that is to load it from disk, where it was saved after each training step, and either run inference on it locally or upload it to a central hub like HuggingFace.\n",
        "\n",
        "Check out the code below for small demo of the model you just trained playing tic tac toe!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2x33xVg8lSxi",
        "outputId": "afce9c5e-957d-4672-b0f5-c6a0d2a887cf"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# example: .art/tic-tac-toe/models/002/checkpoints/0003\u001b[39;00m\n\u001b[1;32m      5\u001b[0m lora_model_path \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.art/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mproject\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/models/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/checkpoints/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28;01mawait\u001b[39;00m\u001b[38;5;250m \u001b[39mmodel\u001b[38;5;241m.\u001b[39mget_step()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m04d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Path(\u001b[38;5;28;01mNone\u001b[39;00m)\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo trained model found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlora_model_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# example: .art/tic-tac-toe/models/002/checkpoints/0003\n",
        "lora_model_path = (\n",
        "    f\".art/{model.project}/models/{model.name}/checkpoints/{await model.get_step():04d}\"\n",
        ")\n",
        "\n",
        "if Path(lora_model_path).exists():\n",
        "    import torch\n",
        "    from unsloth import FastLanguageModel\n",
        "\n",
        "    print(f\"loading model from {lora_model_path}\\n\")\n",
        "\n",
        "    peft_model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=lora_model_path,\n",
        "        max_seq_length=16384,\n",
        "        dtype=torch.bfloat16,\n",
        "        load_in_4bit=True,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(peft_model)\n",
        "\n",
        "    game = generate_game()\n",
        "    move_number = 0\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": f\"You are a tic-tac-toe player. You are playing against an opponent. Always choose the move most likely to lead to an eventual win. Return your move as an XML object with a single property 'move', like so: <move>A1</move>. Optional moves are 'A1', 'B3', 'C2', etc. You are the {game['agent_symbol']} symbol.\",\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    if game[\"agent_symbol\"] == \"o\":\n",
        "        starting_opponent_move = get_opponent_move(game)\n",
        "        game[\"board\"][starting_opponent_move[0]][starting_opponent_move[1]] = game[\n",
        "            \"opponent_symbol\"\n",
        "        ]\n",
        "\n",
        "    while check_winner(game[\"board\"]) is None:\n",
        "        rendered_board = render_board(game)\n",
        "        messages.append({\"role\": \"user\", \"content\": rendered_board})\n",
        "\n",
        "        inputs = tokenizer.apply_chat_template(\n",
        "            messages, return_tensors=\"pt\", add_generation_prompt=True\n",
        "        ).to(\"cuda\")\n",
        "\n",
        "        content = \"\"\n",
        "\n",
        "        def get_completion() -> str:\n",
        "            with torch.no_grad():\n",
        "                outputs = peft_model.generate(\n",
        "                    input_ids=inputs,\n",
        "                    max_new_tokens=100,\n",
        "                    do_sample=True,\n",
        "                    temperature=0.7,\n",
        "                    top_p=0.9,\n",
        "                )\n",
        "                return tokenizer.decode(\n",
        "                    outputs[0][inputs.shape[1] :], skip_special_tokens=True\n",
        "                )\n",
        "\n",
        "        try:\n",
        "            content = get_completion()\n",
        "        except Exception as e:\n",
        "            print(\"caught exception generating chat completion\", e)\n",
        "            raise e\n",
        "\n",
        "        messages.append({\"role\": \"assistant\", \"content\": content})\n",
        "\n",
        "        try:\n",
        "            apply_agent_move(game, content)\n",
        "            move_number += 1\n",
        "        except ValueError as e:\n",
        "            print(f\"Invalid move on move {move_number}: {content}\")\n",
        "            print(f\"Reason: {e}\")\n",
        "            continue\n",
        "\n",
        "        # print the board every move\n",
        "        print(f\"\\nmove {move_number}\")\n",
        "        print(f\"board:\\n{rendered_board}\")\n",
        "        print(f\"agent move: {content}\")\n",
        "        print(f\"updated board:\\n{render_board(game)}\")\n",
        "\n",
        "        if check_winner(game[\"board\"]) is not None:\n",
        "            break\n",
        "        move_number += 1\n",
        "\n",
        "        opponent_move = get_opponent_move(game)\n",
        "        game[\"board\"][opponent_move[0]][opponent_move[1]] = game[\"opponent_symbol\"]\n",
        "\n",
        "    winner = check_winner(game[\"board\"])\n",
        "\n",
        "    print(f\"game finished in {move_number} moves\")\n",
        "\n",
        "    if winner == game[\"agent_symbol\"]:\n",
        "        print(\"game won! üí™\")\n",
        "    elif winner == game[\"opponent_symbol\"]:\n",
        "        print(\"game lost! üò¢\")\n",
        "    elif winner == \"draw\":\n",
        "        print(\"draw! ü§∑‚Äç‚ôÇÔ∏è\")\n",
        "\n",
        "    print(f\"final board:\\n\\n{render_board(game)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iz8PL7zalSxk"
      },
      "source": [
        "<div class=\"align-center\">\n",
        "<a href=\"https://github.com/openpipe/art\"><img src=\"https://github.com/openpipe/art/raw/main/assets/ART_pill.png\" height=\"50\"></a>\n",
        "<a href=\"https://discord.gg/zbBHRUpwf4\"><img src=\"https://github.com/openpipe/art/raw/main/assets/Discord.png\" height=\"50\"></a>\n",
        "<a href=\"https://openpipe.ai/blog/art-e-mail-agent\"><img src=\"https://github.com/openpipe/art/raw/main/assets/ART_E_pill.png\" height=\"50\"></a>\n",
        "\n",
        "Questions? Join the Discord and ask away! For feature requests or to leave a star, visit our [Github](https://github.com/openpipe/art).\n",
        "\n",
        "</div>\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}